<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>LLM Inference Primer (or fastest ramp-up to FlashInfer) | Pratyush Das</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Blog by Pratyush Das">
    <link rel="stylesheet" href="/css/main.css">
  </head>
  <body>
    <article class="blog-post">
  <header class="post-header">
    <h1 class="post-title">LLM Inference Primer (or fastest ramp-up to FlashInfer)</h1>
    
    <time class="post-date">January 21, 2026</time>
    
  </header>
  <div class="post-content">
    <h2>Sequence Representation</h2>
<p>A <strong>prompt</strong> is a sequence of text. The sequence is broken into <strong>tokens</strong> (tokenization is implementation-specific). Each token is converted into a vector representation of size <code>d_model</code> (an embedding).</p>
<p>Positional information is also encoded into each token’s vector (e.g., via sinusoidal functions or learned embeddings), so the model knows where each token appears in the sequence. The specifics vary by model, but the result is the same: each token’s embedding encodes both what the token is and where it appears.</p>
<p>Let <code>seq_len</code> be the number of tokens in a sequence. The sequence is represented as a dense matrix of shape <code>(seq_len, d_model)</code>, where each row is the embedding of a token.</p>
<p>Let’s call this sequence matrix <strong>X</strong>:</p>
<pre><code>     [  ]
     [  ]
X =  [  ]  (seq_len × d_model)
     [  ]
     [  ]
</code></pre>
<h2>Attention Mechanism<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></h2>
<p>The pre-trained model provides weight matrices: <strong>W_Q</strong>, <strong>W_K</strong>, <strong>W_V</strong>, and <strong>W_vocab</strong>. These are constant for the entire inference. (How are they generated? There will be a training primer later :p)</p>
<p><strong>Query, Key, and Value projections:</strong></p>
<ul>
<li>Q = X · W_Q</li>
<li>K = X · W_K</li>
<li>V = X · W_V</li>
</ul>
<p><strong>Attention computation:</strong></p>
<ul>
<li>A = Q · K^T<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></li>
<li>A = mask(A)  (zero out A[i, j] where j &gt; i - a future token j cannot contribute to a previous token i; this is required for correctness, not just performance)</li>
<li>A = softmax(A)  (normalize each row to sum to 1)</li>
</ul>
<p><strong>Why softmax?</strong> After softmax, each row of A sums to 1, meaning A[i, j] represents the fraction of token j’s value vector that contributes to token i’s representation.</p>
<p><strong>Output:</strong></p>
<ul>
<li>Z = A · V</li>
<li>Z = X + Z  (residual connection — each token’s original representation carries through)</li>
</ul>
<p>Z is the attention output.</p>
<p><strong>Implementation detail:</strong> In practice, the attention computation above is repeated multiple times in sequence, each with its own set of W_Q, W_K, W_V weight matrices. The output Z of one repetition becomes the input X to the next. The final repetition’s output is used for token prediction.</p>
<h2>Token Prediction</h2>
<p>For predicting the next token, we use only the last row of Z (the representation of the last token in the existing sequence):</p>
<ul>
<li>logits = Z[-1] · W_vocab  (vector of shape <code>vocab_size</code> - now each element is a token, instead of each row being a token)</li>
<li>probabilities = softmax(logits)  (vector of shape <code>vocab_size</code> where each element is the probability that the corresponding vocabulary token is the next token; all elements sum to 1)</li>
<li>next_token = sample(probabilities)  (sample a token from this probability distribution)</li>
</ul>
<p><strong>Temperature</strong> controls how the probability distribution is shaped before sampling. The logits are divided by a temperature parameter T before softmax: <code>probabilities = softmax(logits / T)</code>. With T &lt; 1, the distribution becomes sharper (the model is more likely to pick the highest-probability token). With T &gt; 1, the distribution becomes flatter (more random). At T → 0, sampling always picks the highest-probability token (greedy decoding).</p>
<h2>Next Generation Step</h2>
<p>For the next round of token prediction:</p>
<ol>
<li>Convert <code>next_token</code> to its embedding vector (size <code>d_model</code>)</li>
<li>Append this new embedding as a new row to X: X_new = [X; next_token_embedding]</li>
<li>X_new now has shape <code>(seq_len + 1, d_model)</code></li>
<li>Re-run the attention mechanism and token prediction with X_new</li>
</ol>
<p>This process repeats, generating one token at a time until a stopping condition (e.g., end-of-sequence token or maximum length).</p>
<!-- Claude: Do we need to briefly mention residual connection here -->
<h2>Query Optimization</h2>
<p>For the first step of inference (called the prompt/prefill phase), we compute Q = X · W_Q for all tokens. However, we only need the last row of Q in subsequent steps (called the autoregressive generation phase/decode phase).</p>
<p>Let <code>X_last</code> be the last token that was predicted and appended to the end of X. For subsequent steps, we can optimize the calculation:</p>
<ul>
<li>Q = X_last · W_Q  (now a vector of shape <code>d_model</code> instead of a matrix)</li>
</ul>
<p>The rest of the computation remains the same:</p>
<ul>
<li>A = mask(Q · K^T)</li>
<li>A = softmax(A)</li>
<li>Z = A · V</li>
</ul>
<h2>KV Cache</h2>
<p>Since W_Q, W_K, W_V, and W_vocab are constant during inference, we can cache previously computed K and V values. When a new token is appended to X, we only need to:</p>
<ol>
<li>Compute K_new and V_new for the new token</li>
<li>Append these to the cached K and V from previous tokens</li>
</ol>
<p>This avoids recomputing K and V for all previous tokens at each step.</p>
<p><strong>Why don’t we cache Q?</strong> As shown in the query optimization above, we only need Q for the last token, so there’s no need to cache the Q matrix.</p>
<p><strong>Insight:</strong> LLM inference mostly involves vector-matrix multiplication instead of matrix-matrix multiplication.</p>
<h2>Batched Inference</h2>
<p>The autoregressive generation phase is <strong>memory-bound</strong>: loading model weights (W_Q, W_K, W_V, W_vocab) from GPU memory dominates the cost, while the actual computation is relatively cheap. Processing one sequence at a time underutilizes the GPU.</p>
<p><strong>Batching</strong> processes multiple sequences simultaneously. Since all sequences share the same model weights, we load the weights once and apply them to all sequences in the batch. This amortizes the memory bandwidth cost and increases GPU utilization. Each sequence in the batch maintains its own KV cache, as they contain different tokens. The sequences do not need to be related.</p>
<p><strong>Shared prefix optimization:</strong> When sequences share a common prefix (e.g., a system prompt), they can potentially share the KV cache for that prefix portion, reducing memory usage.</p>
<h2>PagedAttention<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></h2>
<p>However, sequences cannot share the KV cache during the autoregressive generation phase. The KV cache is typically stored as a contiguous block of memory. Consider two prompts A and B: once they diverge beyond their shared prefix, A and B require different KV cache values. Since the KV cache is stored in contiguous memory, A and B cannot write to the existing shared KV cache and must maintain separate copies.</p>
<p>Additionally, traditional systems suffer from memory fragmentation. In such systems:</p>
<ul>
<li>A request arrives, but the system does not know the output length</li>
<li>The system pre-allocates contiguous memory for the maximum possible length (e.g., 2048 tokens)</li>
<li>The actual sequence ends up being 500 tokens</li>
<li>1548 slots are wasted (internal fragmentation)</li>
<li>Different requests have different maximum lengths, creating gaps between allocations (external fragmentation). Free memory may exist, but if it is not contiguous, a new KV cache cannot be allocated. One might consider using pointer-based allocation (as CPU allocators like <code>malloc</code> do) to utilize scattered free memory, but this degrades GPU performance—GPUs are optimized for coalesced memory access, and random indirection through pointers for each KV cache access is expensive. <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></li>
<li>Result: Very low percentage of KV cache memory actually stores useful data</li>
</ul>
<p>PagedAttention addresses these issues by splitting the KV cache into fixed-size blocks of non-contiguous memory, with a block table that enables lookup into each block.</p>
<p>PagedAttention:</p>
<ul>
<li>Allocates blocks on demand as tokens are generated</li>
<li>A sequence using 500 tokens allocates exactly ⌈500/B⌉ blocks</li>
<li>All blocks have the same size, eliminating external fragmentation</li>
<li>The only waste is unfilled slots in the last block</li>
<li>Result: High memory utilization</li>
</ul>
<p>This block-based indirection also enables sequences A and B to share the same KV cache blocks for their common prefix.</p>
<p><strong>Example:</strong> Consider two prompts with a shared prefix (using page size = 1 token):</p>
<ul>
<li>Prompt A: “The cat sat” (3 tokens)</li>
<li>Prompt B: “The cat ran” (3 tokens)</li>
</ul>
<p>“The” and “cat” are shared. “sat” is unique to A, “ran” is unique to B.</p>
<pre><code>Block tables:
  A: [P0, P1, P2]
  B: [P0, P1, P3]   ← P0, P1 shared via reference counting

Physical pages:
  P0: &quot;The&quot;  (shared)
  P1: &quot;cat&quot;  (shared)
  P2: &quot;sat&quot;  (A-only)
  P3: &quot;ran&quot;  (B-only)
</code></pre>
<h2>RadixAttention<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></h2>
<p>PagedAttention’s primary motivation was solving memory fragmentation—its contribution is efficient memory management for concurrent requests. It could theoretically retain KV cache after requests complete for future reuse, but its per-sequence block tables don’t provide a way to find prefix matches: when a new request arrives, how do you efficiently determine if any cached prefix matches?</p>
<p>RadixAttention solves this with a <strong>radix tree</strong>: a global data structure for cross-request KV cache reuse over time. The tree works as follows:</p>
<ul>
<li>Each <strong>edge</strong> is labeled with a sequence of tokens (compressed representation—edges with no branching are merged)</li>
<li>Each <strong>node</strong> stores a pointer to the KV cache blocks for the token sequence from root to that node</li>
<li>To find a matching prefix, traverse from the root following edges that match the request’s tokens</li>
<li>The traversal finds the longest matching prefix in O(n) time (where n is the request length)</li>
<li>An LRU eviction policy removes least-recently-used leaf nodes when memory fills up</li>
</ul>
<p><strong>Radix Tree:</strong> When a new sequence partially matches an existing edge, the radix tree splits the edge. For example: if the tree has edge [A,B,C,D,E] and a new request needs [A,B,C,F,G], the edge splits into [A,B,C] → node → [D,E], with a new edge [F,G] branching from that node.</p>
<p>RadixAttention reuses the existing KV cache for the shared prefix [A,B,C] and only computes new KV cache blocks for the divergent suffix [F,G].</p>
<p><strong>Example:</strong> Using the same prompts as the PagedAttention example:</p>
<pre><code>Root
  └─&quot;The&quot;─→ [R0]
              └─&quot;cat&quot;─→ [R1]
                         ├─&quot;sat&quot;─→ [R2]
                         └─&quot;ran&quot;─→ [R3]

A uses: R0 → R1 → R2
B uses: R0 → R1 → R3
</code></pre>
<p>Contiguous KV cache storage cannot support this. A radix tree requires sharing and extending prefixes dynamically—if request A caches tokens [1,2,3] and request B needs [1,2,3,4,5], B must extend A’s cache without copying it. Contiguous storage would require pre-allocating space or copying the entire prefix.</p>
<p>PagedAttention and RadixAttention are two points in a much larger space of KV cache optimizations — covering memory efficiency, prefix reuse, speculative decoding, quantized caches, and distributed storage across nodes.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> We focus on these two because they directly motivate the attention kernel design in FlashInfer.</p>
<h2>FlashInfer<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></h2>
<p>Both PagedAttention and RadixAttention store the KV-cache in non-contiguous memory, but their data structures differ. FlashInfer uses a block sparse matrix as a lookup table that can represent both styles (and more).</p>
<p>The block sparse matrix maps query positions (rows) to KV positions (columns). Each row stores which physical pages the kernel needs to load K and V from to compute attention (recall: K = X · W_K, V = X · W_V from Attention Mechanism, and only K and V are cached — see KV Cache). Q is computed on the fly for each position (see Query Optimization). During batched inference, queries from all requests are packed into a single contiguous array without padding (a ragged tensor). Consider a variation of the earlier example with different-length requests:</p>
<pre><code>Prompt A: &quot;The cat sat&quot;      (3 tokens)
Prompt B: &quot;The cat ran fast&quot; (4 tokens)

Without ragged tensor (padded to max length):
  A: [Q_&quot;The&quot;, Q_&quot;cat&quot;, Q_&quot;sat&quot;, &lt;pad&gt; ]
  B: [Q_&quot;The&quot;, Q_&quot;cat&quot;, Q_&quot;ran&quot;, Q_&quot;fast&quot;]
  → stored as a 2 × 4 matrix, wasting one slot for A

Ragged tensor (packed, no padding):
  data:   [Q_&quot;The&quot;, Q_&quot;cat&quot;, Q_&quot;sat&quot;, Q_&quot;The&quot;, Q_&quot;cat&quot;, Q_&quot;ran&quot;, Q_&quot;fast&quot;]
           ← request A (3 tokens) →    ← request B (4 tokens) →
  indptr: [0, 3, 7]
</code></pre>
<p><code>indptr</code> marks where each request starts and ends: request A = <code>data[0:3]</code>, request B = <code>data[3:7]</code>. Each entry is a vector of shape <code>d_model</code>. The rows of the block sparse matrix correspond to positions in this packed array.</p>
<p>The matrix is divided into blocks of size (Br, Bc):</p>
<ul>
<li><strong>Br</strong> (block row size) = query tile size: how many query positions per row block. Br=1 during the decode phase (one new token), larger during the prefill phase.</li>
<li><strong>Bc</strong> (block column size) = KV page size: how many KV positions per column block. Determined by the KV-cache management algorithm.</li>
</ul>
<p><strong>Example:</strong> Using d_model = 2 for illustration (recall d_model from Sequence Representation). “The” and “cat” are the shared prefix. “sat” is unique to A, “ran” and “fast” are unique to B.</p>
<p>KV cache pages (page size = 1 token, same layout as the PagedAttention example):</p>
<pre><code>  P0 (&quot;The&quot;, shared):   K=[1,0],  V=[1,1]
  P1 (&quot;cat&quot;, shared):   K=[0,1],  V=[2,0]
  P2 (&quot;sat&quot;, A-only):   K=[1,1],  V=[0,1]
  P3 (&quot;ran&quot;, B-only):   K=[1,-1], V=[1,0]
  P4 (&quot;fast&quot;, B-only):  K=[0,-1], V=[0,1]
</code></pre>
<p><strong>FlashInfer’s block sparse matrix</strong> with Br=1, Bc=1. Each row is one query position, each column is one KV page. Each cell stores the page index to load K, V from. Empty cells mean “no attention”:</p>
<pre><code>              P0     P1     P2     P3     P4
              (The)  (cat)  (sat)  (ran)  (fast)
             shared shared A-only B-only B-only
            ┌──────┬──────┬──────┬──────┬──────┐
A, pos 0    │  P0  │      │      │      │      │
            ├──────┼──────┼──────┼──────┼──────┤
A, pos 1    │  P0  │  P1  │      │      │      │
            ├──────┼──────┼──────┼──────┼──────┤
A, pos 2    │  P0  │  P1  │  P2  │      │      │
            ├──────┼──────┼──────┼──────┼──────┤
B, pos 0    │  P0  │      │      │      │      │
            ├──────┼──────┼──────┼──────┼──────┤
B, pos 1    │  P0  │  P1  │      │      │      │
            ├──────┼──────┼──────┼──────┼──────┤
B, pos 2    │  P0  │  P1  │      │  P3  │      │
            ├──────┼──────┼──────┼──────┼──────┤
B, pos 3    │  P0  │  P1  │      │  P3  │  P4  │
            └──────┴──────┴──────┴──────┴──────┘
</code></pre>
<p>This matrix is stored as a lookup table — for each row, it records which columns (physical pages) to access:</p>
<pre><code>A, pos 0 → pages [P0]
A, pos 1 → pages [P0, P1]
A, pos 2 → pages [P0, P1, P2]
B, pos 0 → pages [P0]
B, pos 1 → pages [P0, P1]
B, pos 2 → pages [P0, P1, P3]
B, pos 3 → pages [P0, P1, P3, P4]
</code></pre>
<p><strong>Kernel workflow for row “A, pos 2”:</strong></p>
<p>Q for position 2 is computed on the fly: Q = [1,1]. The lookup table says: load KV from pages P0, P1, P2.</p>
<pre><code>1. Load K from P0, P1, P2:  [[1,0], [0,1], [1,1]]
2. Attention scores = Q · K^T:
     [1,1]·[1,0] = 1,  [1,1]·[0,1] = 1,  [1,1]·[1,1] = 2
     → scores = [1, 1, 2]
3. Causal mask: all valid (positions 0, 1, 2 are all ≤ 2)
4. Softmax([1, 1, 2]) = [0.21, 0.21, 0.58]
5. Load V from P0, P1, P2:  [[1,1], [2,0], [0,1]]
6. Output = 0.21·[1,1] + 0.21·[2,0] + 0.58·[0,1] = [0.63, 0.79]
</code></pre>
<p><strong>Causal mask in the Br=1 case:</strong> In step 3, the causal mask didn’t mask anything — all loaded KV positions (0, 1, 2) were ≤ the query position (2). This is true for every row: “A, pos 0” only loads P0, “A, pos 1” loads P0 and P1, etc. With Br=1, the lookup table already encodes causality by only listing pages up to each query’s position, making the causal mask a no-op.</p>
<p><strong>Optimization: Br=3.</strong> During prefill, query positions within each request can be grouped into row blocks. Request A (3 tokens) fits in one block of Br=3. Request B (4 tokens) requires ⌈4/3⌉ = 2 blocks: [pos 0-2] and [pos 3]. Each filled block represents multiple query positions looking up KV from the same pages:</p>
<pre><code>              P0     P1     P2     P3     P4
              (The)  (cat)  (sat)  (ran)  (fast)
             shared shared A-only B-only B-only
            ┌──────┬──────┬──────┬──────┬──────┐
A, pos 0-2  │  P0  │  P1  │  P2  │      │      │
            ├──────┼──────┼──────┼──────┼──────┤
B, pos 0-2  │  P0  │  P1  │      │  P3  │      │
            ├──────┼──────┼──────┼──────┼──────┤
B, pos 3    │  P0  │  P1  │      │  P3  │  P4  │
            └──────┴──────┴──────┴──────┴──────┘
</code></pre>
<p>The kernel loads Q for all positions in a block at once and reuses the loaded K, V across all queries in the block. The causal mask is applied inside each block:</p>
<pre><code>Scores for A, pos 0-2:
              P0     P1     P2
   pos 0    [ 1     -∞     -∞  ]  ← pos 0 can only attend to pos 0
   pos 1    [ 0      1     -∞  ]  ← pos 1 can attend to pos 0, 1
   pos 2    [ 1      1      2  ]  ← pos 2 can attend to all
</code></pre>
<p><strong>Causal mask in the Br=3 case:</strong> Unlike Br=1, the causal mask is now essential. All 3 of A’s positions are in one row block and load the same pages (P0, P1, P2). Without the mask, position 0 would incorrectly attend to positions 1 and 2. The -∞ entries ensure these scores become 0 after softmax. The tradeoff: larger Br loads K, V once for multiple queries (more efficient), but some loaded values get masked out (wasted work within the block).</p>
<p>Same results as Br=1, but more efficient — K, V are loaded once and reused.</p>
<p><strong>Optimization: Bc=2.</strong> With a page size of 2, each page stores KV for 2 token positions. The KV cache is repacked:</p>
<pre><code>  P0 (&quot;The&quot;+&quot;cat&quot;, shared):       K=[[1,0],[0,1]],   V=[[1,1],[2,0]]
  P1 (&quot;sat&quot;, A-only):             K=[[1,1]],          V=[[0,1]]
  P2 (&quot;ran&quot;+&quot;fast&quot;, B-only):      K=[[1,-1],[0,-1]],  V=[[1,0],[0,1]]
</code></pre>
<p>Each filled block is now a (1 × 2) lookup — 1 query position loading KV for 2 token positions at once:</p>
<pre><code>              P0           P1       P2
           (The, cat)    (sat)    (ran, fast)
             shared      A-only   B-only
            ┌──────────┬────────┬──────────┐
A, pos 0    │    P0    │        │          │
            ├──────────┼────────┼──────────┤
A, pos 1    │    P0    │        │          │
            ├──────────┼────────┼──────────┤
A, pos 2    │    P0    │   P1   │          │
            ├──────────┼────────┼──────────┤
B, pos 0    │    P0    │        │          │
            ├──────────┼────────┼──────────┤
B, pos 1    │    P0    │        │          │
            ├──────────┼────────┼──────────┤
B, pos 2    │    P0    │        │    P2    │
            ├──────────┼────────┼──────────┤
B, pos 3    │    P0    │        │    P2    │
            └──────────┴────────┴──────────┘
</code></pre>
<p>Fewer blocks to iterate over, but some loaded KV entries are masked out. For example, when A, pos 0 loads from P0, it gets K, V for both “The” and “cat” — but the causal mask zeroes out the “cat” score since position 1 &gt; position 0.</p>
<p><strong>Optimization: composable formats.</strong> Consider what happens with the Br=1, Bc=1 matrix when processing the shared prefix. All 7 query positions (A’s 3 + B’s 4) independently load KV from P0. That’s K=[1,0], V=[1,1] loaded from global memory 7 separate times. Same for P1. This is wasteful.</p>
<p>FlashInfer can split the attention computation across multiple block sparse matrices with different block sizes and compose the results:</p>
<ul>
<li><strong>Shared prefix matrix</strong> (Br=7, Bc=1): groups all 7 queries into one row block. K, V for each shared page is loaded into fast shared memory once, and all 7 queries compute their attention scores against it.</li>
<li><strong>Unique suffix matrix</strong> (Br=1, Bc=1): each request’s unique suffix tokens are handled independently.</li>
<li></li>
</ul>
<pre><code>Shared prefix matrix (Br=7, Bc=1):

              P0     P1
             (The)  (cat)
            ┌──────┬──────┐
A, pos 0    │      │      │
A, pos 1    │      │      │
A, pos 2    │  P0  │  P1  │  ← one row block, all 7 queries
B, pos 0    │      │      │
B, pos 1    │      │      │
B, pos 2    │      │      │
B, pos 3    │      │      │
            └──────┴──────┘

Kernel for P0 block: load K=[1,0], V=[1,1] once into shared
memory. All 7 queries compute scores against this K and weight
this V — 7× reuse vs. 7 independent global memory loads.
</code></pre>
<pre><code>Unique suffix matrix (Br=1, Bc=1):

              P2     P3     P4
             (sat)  (ran)  (fast)
            ┌──────┬──────┬──────┐
A, pos 2    │  P2  │      │      │
            ├──────┼──────┼──────┤
B, pos 2    │      │  P3  │      │
            ├──────┼──────┼──────┤
B, pos 3    │      │  P3  │  P4  │
            └──────┴──────┴──────┘

Only A pos 2, B pos 2, and B pos 3 have unique suffixes. Other
positions (A pos 0, A pos 1, B pos 0, B pos 1) use only their
shared prefix output.
</code></pre>
<p>Each matrix produces a partial attention output and a log-sum-exp (LSE) normalizer for each query position. These are merged exactly:</p>
<pre><code>For each query position:
  O_shared, LSE_shared = partial attention output from shared prefix matrix
  O_unique, LSE_unique = partial attention output from unique suffix matrix

  LSE_merged = log(exp(LSE_shared) + exp(LSE_unique))
  O_merged   = (exp(LSE_shared) · O_shared + exp(LSE_unique) · O_unique)
                / exp(LSE_merged)
</code></pre>
<p>This merge is mathematically equivalent to computing attention over all KV positions at once — splitting the softmax denominator across subsets and recombining is exact, not an approximation.</p>
<p><strong>Fewer memory loads:</strong> Without composable formats, K, V from each shared page is loaded from global memory 7 times (once per query). With the shared prefix matrix (Br=7), K, V is loaded once into shared memory and reused across all 7 queries.</p>
<p>More importantly, the matrix representation enables composable formats (above). Neither PagedAttention nor RadixAttention can do this on their own: PagedAttention has no mechanism to group queries sharing a prefix into a larger kernel tile, and RadixAttention identifies shared prefixes via its radix tree but each query still loads the shared KV from global memory independently. FlashInfer’s block sparse matrix makes the sharing pattern explicit in the sparsity structure, enabling the kernel to load shared KV once and reuse it across queries.</p>
<h2>FlashInfer’s plan/run API</h2>
<p>FlashInfer exposes a two-phase API for attention computation: <code>plan()</code> and <code>run()</code>.</p>
<p><code>plan()</code> runs on the CPU before each generation step. It takes <code>kv_indptr</code> and <code>kv_indices</code> — the indirection arrays encoding the block sparse matrix — as inputs, and computes a schedule: how to partition work across sequences, and how to parallelize across the KV dimension.</p>
<p><code>run()</code> launches the GPU kernel, passing <code>kv_indptr</code> and <code>kv_indices</code> directly to it. <code>kv_indptr</code> marks where each sequence’s page list begins and ends (the same indptr structure used for the ragged query tensor); <code>kv_indices</code> lists the physical page indices for each sequence. The kernel is parameterized on these arrays — a single generic kernel handles any block sparse structure by following the indirection at runtime.</p>
<p>The generation loop therefore looks like this (Listing 1 of the FlashInfer paper<sup class="footnote-ref"><a href="#fn7" id="fnref7:1">[7:1]</a></sup>):</p>
<pre><code class="language-python">attn.plan(batch_structure)      # one-time setup
with torch.cuda.graph(g):
    attn.run(q, kv_cache)       # capture GPU kernel

while not finished:
    attn.plan(batch_structure)  # CPU: recompute schedule
    g.replay()                  # GPU: execute kernel
</code></pre>
<p><code>plan()</code> is called every step because the batch structure changes: each generated token extends every sequence by one, potentially adding a new KV page.</p>
<h2>SGLang and FlashInfer</h2>
<p>SGLang<sup class="footnote-ref"><a href="#fn5" id="fnref5:1">[5:1]</a></sup> uses RadixAttention (see <a href="#radixattention">RadixAttention</a>) to allocate and track KV cache pages, using the radix tree to identify which pages are shared across sequences. SGLang computes <code>kv_indptr</code> and <code>kv_indices</code> — the block sparse matrix encoding which physical KV pages each sequence attends to — and passes them to FlashInfer’s <code>plan()</code> and <code>run()</code> at each decode step to execute the attention computation.</p>
<hr>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>In practice, A = (Q · K^T) / √d_k is used where √d_k is a scaling factor. This scaling is for numerical stability: as the dimension d_k grows, the dot products can become very large. The scaling keeps the variance of the dot products constant, preventing this issue. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., &amp; Stoica, I. (2023). <a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a>. <em>Proceedings of the 29th Symposium on Operating Systems Principles (SOSP)</em>. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>PagedAttention also introduces indirection, but at block granularity rather than element granularity. The block table is consulted once per block to find its physical location, then memory access within the block is contiguous and coalesced. This amortizes the indirection cost over many elements, unlike fine-grained pointer chasing for every KV cache access. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., &amp; Sheng, Y. (2024). <a href="https://arxiv.org/abs/2312.07104">SGLang: Efficient Execution of Structured Language Model Programs</a>. <em>Advances in Neural Information Processing Systems 37 (NeurIPS)</em>. <a href="#fnref5" class="footnote-backref">↩︎</a> <a href="#fnref5:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Zhang, B. (2026). <a href="https://www.modular.com/blog/the-five-eras-of-kvcache">The Five Eras of KVCache</a>. <em>Modular Blog</em>. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., Chen, T., Kasikci, B., Grover, V., Krishnamurthy, A., &amp; Ceze, L. (2025). <a href="https://arxiv.org/abs/2501.01005">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a>. <em>Proceedings of Machine Learning and Systems (MLSys)</em>. <a href="#fnref7" class="footnote-backref">↩︎</a> <a href="#fnref7:1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

  </div>
  <footer class="post-footer">
    <a href="/">&larr; Back to Home</a>
  </footer>
</article>

  </body>
</html>
